{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  AWS Glue session config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "vscode": {
     "languageId": "python_glue_session"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Current session configs: <tt>{'conf': {'spark.serializer': 'org.apache.spark.serializer.KryoSerializer', 'spark.sql.hive.convertMetastoreParquet': 'false', 'spark.sql.catalog.spark_catalog': 'org.apache.spark.sql.hudi.catalog.HoodieCatalog', 'spark.sql.legacy.pathOptionBehavior.enabled': 'true', 'spark.sql.extensions': 'org.apache.spark.sql.hudi.HoodieSparkSessionExtension'}, 'kind': 'pyspark'}</tt><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "No active sessions."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%configure -f\n",
    "{\n",
    "    \"conf\": {\n",
    "        \"spark.serializer\": \"org.apache.spark.serializer.KryoSerializer\",\n",
    "        \"spark.sql.hive.convertMetastoreParquet\": \"false\",\n",
    "        \"spark.sql.catalog.spark_catalog\": \"org.apache.spark.sql.hudi.catalog.HoodieCatalog\",\n",
    "        \"spark.sql.legacy.pathOptionBehavior.enabled\": \"true\",\n",
    "        \"spark.sql.extensions\": \"org.apache.spark.sql.hudi.HoodieSparkSessionExtension\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Local bucket config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": true,
    "tags": [],
    "vscode": {
     "languageId": "python_glue_session"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>12</td><td>None</td><td>pyspark</td><td>idle</td><td></td><td></td><td>None</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "try:\n",
    "    import os, uuid, sys, boto3, time, sys\n",
    "    from pyspark.context import SparkContext\n",
    "    from pyspark.sql.functions import lit, udf\n",
    "    from pyspark.sql.session import SparkSession\n",
    "    from awsglue.context import GlueContext\n",
    "    from awsglue.transforms import *\n",
    "    from awsglue.utils import getResolvedOptions\n",
    "    from awsglue.job import Job\n",
    "except Exception as e:\n",
    "    print(\"Modules are missing : {} \".format(e))\n",
    "\n",
    "\n",
    "spark._jsc.hadoopConfiguration().set(\"fs.s3a.endpoint\", \"http://minio:9000/\")\n",
    "spark._jsc.hadoopConfiguration().set(\"fs.s3a.access.key\", \"minioadmin\")\n",
    "spark._jsc.hadoopConfiguration().set(\"fs.s3a.secret.key\", \"minioadmin\")\n",
    "spark._jsc.hadoopConfiguration().set(\"fs.s3a.path.style.access\", \"true\")\n",
    "spark._jsc.hadoopConfiguration().set(\"fs.s3a.connection.ssl.enabled\", \"false\")\n",
    "spark._jsc.hadoopConfiguration().set(\"fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "spark._jsc.hadoopConfiguration().set(\"fs.s3a.aws.credentials.provider\",\"org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider\")\n",
    "\n",
    "\n",
    "# sc = SparkSession.builder.getOrCreate()\n",
    "# glueContext = GlueContext(sc)\n",
    "# job = Job(glueContext)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Import demo data applying its schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "vscode": {
     "languageId": "python_glue_session"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(id='99e7a457-c86d-47de-824e-dcb90865a388', type='debit', created_at=datetime.datetime(2022, 9, 13, 0, 0), document='80', payer='Joyce Wimberly', amount=3814)]"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, TimestampType\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"id\", StringType(), True),\n",
    "    StructField(\"type\", StringType(), True),\n",
    "    StructField(\"created_at\", TimestampType(), True),\n",
    "    StructField(\"document\", StringType(), True),\n",
    "    StructField(\"payer\", StringType(), True),\n",
    "    StructField(\"amount\", IntegerType(), True),\n",
    "])\n",
    "\n",
    "df = spark.read.format(\"json\") \\\n",
    "    .schema(schema) \\\n",
    "    .load(\"s3a://warehouse/raw\")\n",
    "\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Hudi configuration builder method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def build_hudi_config(\n",
    "    database, \n",
    "    table, \n",
    "    record_id, \n",
    "    precomb_key, \n",
    "    table_type, \n",
    "    partition_fields,\n",
    "    operation,\n",
    "    enable_partition, \n",
    "    enable_cleaner, \n",
    "    enable_hive_sync, \n",
    "    enable_clustering,\n",
    "    enable_metadata_indexing, \n",
    "    index_type='BLOOM', \n",
    "    clustering_column='default'\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Upserts a dataframe into a Hudi table.\n",
    "\n",
    "    Args:\n",
    "        database (str):                     Name of the glue database.\n",
    "        table (str):                        Name of the Hudi table.\n",
    "        record_id (str):                    Field in the dataframe that will be used as the record key.\n",
    "        precomb_key (str):                  Field in the dataframe that will be used for pre-combine.\n",
    "        table_type (str):                   COPY_ON_WRITE or MERGE_ON_READ.\n",
    "        partition_fields(str):              Filds used in the partitioning criteria \n",
    "        operation (str):                    The Hudi write method to use.\n",
    "        enable_partition (bool):            Whether or not to enable partitioning.\n",
    "        enable_cleaner (bool):              Whether or not to enable data cleaning.\n",
    "        enable_hive_sync (bool):            Whether or not to enable syncing with Hive.\n",
    "        enable_clustering (bool):           Whether or not to enable clustering.\n",
    "        enable_metadata_indexing (bool):    Whether or not to enable metadata indexing.\n",
    "        index_type :                        BLOOM or GLOBAL_BLOOM\n",
    "    Returns:\n",
    "        Dict\n",
    "    \"\"\"\n",
    "    # These are the basic settings for the Hoodie table\n",
    "    hudi_base_settings = {\n",
    "        \"hoodie.table.name\": table,\n",
    "        \"hoodie.datasource.write.table.type\": table_type,\n",
    "        \"hoodie.datasource.write.operation\": operation,\n",
    "        \"hoodie.datasource.write.recordkey.field\": record_id,\n",
    "        \"hoodie.datasource.write.precombine.field\": precomb_key,\n",
    "        \"hoodie.index.type\": index_type,\n",
    "        \"hoodie.parquet.max.file.size\": 512 * 1024 * 1024,    # 512MB\n",
    "        \"hoodie.parquet.small.file.limit\": 100 * 1024 * 1024, # 100MB\n",
    "    }\n",
    "\n",
    "    # These settings enable syncing with Hive\n",
    "    hudi_hive_sync_settings = {\n",
    "        \"hoodie.parquet.compression.codec\": \"gzip\",\n",
    "        \"hoodie.datasource.hive_sync.enable\": \"true\",\n",
    "        \"hoodie.datasource.hive_sync.database\": database,\n",
    "        \"hoodie.datasource.hive_sync.table\": table,\n",
    "        \"hoodie.datasource.hive_sync.partition_extractor_class\": \"org.apache.hudi.hive.MultiPartKeysValueExtractor\",\n",
    "        \"hoodie.datasource.hive_sync.use_jdbc\": \"false\",\n",
    "        \"hoodie.datasource.hive_sync.mode\": \"hms\",\n",
    "    }\n",
    "\n",
    "    if enable_hive_sync == True:\n",
    "        for key, value in hudi_hive_sync_settings.items(): hudi_base_settings[key] = value\n",
    "\n",
    "    # These settings enable automatic cleaning of old data\n",
    "    hudi_cleaner_options = {\n",
    "        \"hoodie.clean.automatic\": \"true\",\n",
    "        \"hoodie.clean.async\": \"true\",\n",
    "        \"hoodie.cleaner.policy\": 'KEEP_LATEST_FILE_VERSIONS',\n",
    "        \"hoodie.cleaner.fileversions.retained\": \"3\",\n",
    "        \"hoodie-conf hoodie.cleaner.parallelism\": '200',\n",
    "        'hoodie.cleaner.commits.retained': 5\n",
    "    }\n",
    "\n",
    "    if enable_cleaner == True:\n",
    "        for key, value in hudi_cleaner_options.items(): hudi_base_settings[key] = value\n",
    "\n",
    "    # These settings enable partitioning of the data\n",
    "    partition_settings = {\n",
    "        \"hoodie.datasource.write.partitionpath.field\": partition_fields,\n",
    "        \"hoodie.datasource.hive_sync.partition_fields\": partition_fields,\n",
    "        \"hoodie.datasource.write.hive_style_partitioning\": \"true\",\n",
    "    }\n",
    "\n",
    "    if enable_partition == True:\n",
    "        for key, value in partition_settings.items(): hudi_base_settings[key] = value\n",
    "\n",
    "    # These settings enable data clustering\n",
    "    hudi_clustering = {\n",
    "        \"hoodie.clustering.execution.strategy.class\": \"org.apache.hudi.client.clustering.run.strategy.SparkSortAndSizeExecutionStrategy\",\n",
    "        \"hoodie.clustering.inline\": \"true\",\n",
    "        \"hoodie.clustering.plan.strategy.sort.columns\": clustering_column,\n",
    "        \"hoodie.clustering.plan.strategy.target.file.max.bytes\": \"1073741824\",\n",
    "        \"hoodie.clustering.plan.strategy.small.file.limit\": \"629145600\"\n",
    "    }\n",
    "\n",
    "    if enable_clustering == True:\n",
    "        for key, value in hudi_clustering.items():\n",
    "            hudi_base_settings[key] = value\n",
    "\n",
    "    # These settings enable metadata indexing\n",
    "    hudi_metadata_indexing = {\n",
    "        \"hoodie.metadata.enable\": \"true\",\n",
    "        \"hoodie.metadata.index.async\": \"true\",\n",
    "        \"hoodie.metadata.index.column.stats.enable\": \"true\",\n",
    "        \"hoodie.metadata.index.check.timeout.seconds\": \"60\",\n",
    "        \"hoodie.write.concurrency.mode\": \"optimistic_concurrency_control\",\n",
    "        \"hoodie.write.lock.provider\": \"org.apache.hudi.client.transaction.lock.InProcessLockProvider\"\n",
    "    }\n",
    "\n",
    "    if enable_metadata_indexing == True:\n",
    "        for key, value in hudi_metadata_indexing.items():\n",
    "            hudi_base_settings[key] = value \n",
    "\n",
    "    return hudi_base_settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "vscode": {
     "languageId": "python_glue_session"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hoodie.table.name': 'transactions', 'hoodie.datasource.write.table.type': 'MERGE_ON_READ', 'hoodie.datasource.write.operation': 'upsert', 'hoodie.datasource.write.recordkey.field': 'id', 'hoodie.datasource.write.precombine.field': 'id', 'hoodie.index.type': 'BLOOM', 'hoodie.parquet.max.file.size': 536870912, 'hoodie.parquet.small.file.limit': 104857600, 'hoodie.clean.automatic': 'true', 'hoodie.clean.async': 'true', 'hoodie.cleaner.policy': 'KEEP_LATEST_FILE_VERSIONS', 'hoodie.cleaner.fileversions.retained': '3', 'hoodie-conf hoodie.cleaner.parallelism': '200', 'hoodie.cleaner.commits.retained': 5, 'hoodie.datasource.write.partitionpath.field': 'document', 'hoodie.datasource.hive_sync.partition_fields': 'document', 'hoodie.datasource.write.hive_style_partitioning': 'true'}"
     ]
    }
   ],
   "source": [
    "database_name = \"gold\"\n",
    "table_name = \"transactions\"\n",
    "s3_path = f\"s3://warehouse/{database_name}/{table_name}\"\n",
    "\n",
    "hudi_config = build_hudi_config(\n",
    "    database                 = database_name, \n",
    "    table                    = table_name, \n",
    "    record_id                = \"id\", \n",
    "    precomb_key              = \"id\", \n",
    "    table_type               = \"MERGE_ON_READ\", \n",
    "    partition_fields         = \"document\",\n",
    "    operation                = \"upsert\",\n",
    "    enable_partition         = True, \n",
    "    enable_cleaner           = True, \n",
    "    enable_hive_sync         = False, \n",
    "    enable_clustering        = False, \n",
    "    enable_metadata_indexing = False, \n",
    "    index_type               = \"BLOOM\", \n",
    "    clustering_column        = \"default\"\n",
    ")\n",
    "\n",
    "hudi_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Query helper method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def run_query(query):\n",
    "    transactions_df = spark.read.format(\"hudi\").load(s3_path)\n",
    "    transactions_df.createOrReplaceTempView(\"transactions\")\n",
    "    return spark.sql(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Create Hudi table from df top 5 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "vscode": {
     "languageId": "python_glue_session"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|       5|\n",
      "+--------+"
     ]
    }
   ],
   "source": [
    "top_df = df.limit(5)\n",
    "top_df.write.format(\"hudi\").options(**hudi_config).mode(\"overwrite\").save(s3_path)\n",
    "\n",
    "run_query(\"SELECT COUNT(*) FROM transactions\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Add top 100 df data to existing table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "vscode": {
     "languageId": "python_glue_session"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|     100|\n",
      "+--------+"
     ]
    }
   ],
   "source": [
    "# As the operation was setted as upsert by id the count should result in 100 instead of 105 \n",
    "top_df = df.limit(100)\n",
    "top_df.write.format(\"hudi\").options(**hudi_config).mode(\"append\").save(s3_path)\n",
    "\n",
    "run_query(\"SELECT COUNT(*) FROM transactions\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Update Hudi table data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': '61a606dd-9f4b-42a2-a95d-1f18f0ed753d', 'type': 'credit', 'created_at': datetime.datetime(2022, 4, 2, 0, 0), 'document': '34', 'payer': 'Sheila Ellard', 'amount': 1933}"
     ]
    }
   ],
   "source": [
    "# Selects de 50th row to update its content\n",
    "row_to_update = df.collect()[50].asDict() \n",
    "row_to_update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(id='61a606dd-9f4b-42a2-a95d-1f18f0ed753d', type='credit', created_at=datetime.datetime(2022, 4, 2, 0, 0), document='34', payer='Sheila Ellard', amount=1001933)"
     ]
    }
   ],
   "source": [
    "row_to_update[\"amount\"] += 1000000\n",
    "\n",
    "df_to_update = spark.createDataFrame(data=[row_to_update], schema=schema)\n",
    "df_to_update.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0--------------------------------------------------------\n",
      " _hoodie_commit_time    | 20240111132422890                      \n",
      " _hoodie_commit_seqno   | 20240111132422890_0_75                 \n",
      " _hoodie_record_key     | 61a606dd-9f4b-42a2-a95d-1f18f0ed753d   \n",
      " _hoodie_partition_path | document=34                            \n",
      " _hoodie_file_name      | 45785df7-c465-4a7d-8013-06752c36ba37-0 \n",
      " id                     | 61a606dd-9f4b-42a2-a95d-1f18f0ed753d   \n",
      " type                   | credit                                 \n",
      " created_at             | 2022-04-02 00:00:00                    \n",
      " document               | 34                                     \n",
      " payer                  | Sheila Ellard                          \n",
      " amount                 | 1001933"
     ]
    }
   ],
   "source": [
    "df_to_update.write.format(\"hudi\").options(**hudi_config).mode(\"append\").save(s3_path)\n",
    "\n",
    "run_query('''\n",
    "    SELECT *\n",
    "    FROM transactions \n",
    "    WHERE id = '61a606dd-9f4b-42a2-a95d-1f18f0ed753d'\n",
    "''').show(1, truncate=False, vertical=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Delete Hudi table data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "vscode": {
     "languageId": "python_glue_session"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|       0|\n",
      "+--------+"
     ]
    }
   ],
   "source": [
    "delete_options = hudi_config\n",
    "delete_options['hoodie.datasource.write.operation'] = 'delete'\n",
    "\n",
    "hard_delete_df = run_query(\"SELECT * FROM transactions WHERE id = '61a606dd-9f4b-42a2-a95d-1f18f0ed753d'\")\n",
    "hard_delete_df.write.format(\"hudi\").options(**delete_options).mode(\"append\").save(s3_path)\n",
    "\n",
    "run_query(\"SELECT COUNT(*) FROM transactions WHERE id = '61a606dd-9f4b-42a2-a95d-1f18f0ed753d'\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python_glue_session"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
